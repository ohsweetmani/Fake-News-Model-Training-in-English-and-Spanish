{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc74281",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fake News Detection in English and Spanish\n",
    "##Pre-processing data sets to be easily processed later\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import simplemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d82b848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Category\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...     Fake\n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...     Fake\n",
       "2  U.S. Secretary of State John F. Kerry said Mon...     Real\n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...     Fake\n",
       "4  It's primary day in New York and front-runners...     Real"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#English Fake News Dataset \n",
    "en_data = pd.read_csv('Original Datasets/English_Dataset_Fake News.csv')\n",
    "en_data = en_data.loc[:, ['text', 'label']]\n",
    "#Rename columns\n",
    "en_data.columns = ['Text', 'Category']\n",
    "#Map Category values to Fake and Real only \n",
    "en_data['Category'] = en_data['Category'].map({'FAKE': 'Fake', 'REAL': 'Real'})\n",
    "#Drop rows where text and category are empty strings...there were non Nan values\n",
    "for i in range(0, len(en_data)):\n",
    "    if (en_data['Text'][i].isspace()): \n",
    "        en_data['Text'][i] = np.nan\n",
    "    if (en_data['Category'][i].isspace()): \n",
    "        en_data['Category'][i] = np.nan      \n",
    "en_data = en_data.dropna().reset_index(drop=True)\n",
    "en_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b5677a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RAE INCLUIRÁ LA PALABRA \"LADY\" EN EL DICCIONAR...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La palabra \"haiga\", aceptada por la RAE La Rea...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YORDI ROSADO ESCRIBIRÁ Y DISEÑARÁ LOS NUEVOS L...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNAM capacitará a maestros para aprobar prueba...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alerta: pretenden aprobar libros escolares con...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Category\n",
       "0  RAE INCLUIRÁ LA PALABRA \"LADY\" EN EL DICCIONAR...     Fake\n",
       "1  La palabra \"haiga\", aceptada por la RAE La Rea...     Fake\n",
       "2  YORDI ROSADO ESCRIBIRÁ Y DISEÑARÁ LOS NUEVOS L...     Fake\n",
       "3  UNAM capacitará a maestros para aprobar prueba...     Real\n",
       "4  Alerta: pretenden aprobar libros escolares con...     Fake"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPANISH Fake News Dataset \n",
    "spn_data = pd.read_excel('Original Datasets/Spanish_Dataset_Fake News.xlsx') \n",
    "spn_data = spn_data.loc[:, ['Text', 'Category']]\n",
    "#Map Category values to Fake and Real only \n",
    "spn_data['Category'] = spn_data['Category'].map({'Fake': 'Fake', 'True': 'Real'})\n",
    "spn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53b22817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\P\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\P\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#English Processing w/ NLTK \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('words') #download list of English words\n",
    "nltk.download('stopwords') #download list of English stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopWords = stopwords.words('english')\n",
    "enWords = set(nltk.corpus.words.words()) \n",
    "\n",
    "#lowercase and remove weird characters from text\n",
    "def lower_char_process(text): \n",
    "    text = text.lower() \n",
    "    #Remove URLS \n",
    "    text = re.sub('https://.*\\s?', '', text)\n",
    "    #Remove any weird characters, contractions\n",
    "    text = re.sub('[^a-zA-Z0-9 \\n\\.]', '', text)\n",
    "    text = text.replace('.', '')\n",
    "    return text \n",
    "\n",
    "#Map nltk part of speech tags to tags recognized by wordnet\n",
    "#Because Lemmatizer is based on wordnet\n",
    "def nltk_pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def lemmatized(tokens): \n",
    "    nltk_tagged = nltk.pos_tag(tokens)\n",
    "    lemmas = [] \n",
    "    for word, tag in nltk_tagged: \n",
    "        tag = nltk_pos_tagger(tag)\n",
    "        if tag is None:\n",
    "            lemmas.append(word)\n",
    "        else: \n",
    "            lemmas.append(lemmatizer.lemmatize(word,tag))\n",
    "    return lemmas\n",
    " \n",
    "def stopWordRemoval(tokens): \n",
    "    stopWordRemoved = []\n",
    "    for token in tokens: \n",
    "        if token in enWords and token not in stopWords: \n",
    "            stopWordRemoved.append(token)\n",
    "    return stopWordRemoved\n",
    "\n",
    "#Combine all steps to process text\n",
    "def processText(df) : \n",
    "    textCol = []\n",
    "    text = df['Text'][0]\n",
    "    for i in range(0, len(df)):\n",
    "        text = df['Text'][i]\n",
    "        #Lowercase and remove weird characters\n",
    "        text = lower_char_process(text)\n",
    "        #Tokenization of data \n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text = tokenizer.tokenize(text)\n",
    "        #Lemmatization of tokens\n",
    "        text = lemmatized(text)\n",
    "        #Remove stop words \n",
    "        text = stopWordRemoval(text)\n",
    "        #Joining the processed text as a WHOLE string again\n",
    "        text = ' '.join(text)\n",
    "        textCol.append(text)\n",
    "    return textCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92eceb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spanish Processing w/ NLTK and Simplemma\n",
    "#NLTK does not provide spanish support for lemmatization...so had to use Simplemma\n",
    "simpLang = simplemma.load_data('es')\n",
    "stopWord_spn = stopwords.words('spanish')\n",
    "\n",
    "def stopWordSpn(tokens): \n",
    "    stopWordRemoved = []\n",
    "    for token in tokens: \n",
    "        #Remove newline characters and 'adj'/'f'\n",
    "        removeChar = ['\\n', 'adj', 'f']\n",
    "        if token not in stopWord_spn and token not in removeChar: \n",
    "            stopWordRemoved.append(token)\n",
    "    return stopWordRemoved    \n",
    "\n",
    "def spanProcessText(df) : \n",
    "    textCol = []\n",
    "    for i in range(0, len(df)):\n",
    "        text = df['Text'][i]\n",
    "        #Some more text processing in spanish bc of how data was...\n",
    "        text = text.replace('*NUMBER*', '')\n",
    "        text = re.sub('[^a-zA-Z0-9 \\n\\.ÁáéÉíÍñÑÓóÚÜúü]', '', text)\n",
    "        #Same processing function used for English data\n",
    "        text = lower_char_process(text)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text = tokenizer.tokenize(text)\n",
    "        #Stopword removal in Spanish\n",
    "        text = stopWordSpn(text)\n",
    "        #lemmatize....in spanish, verb+definite article can be used as one word so lemmatization breaks them up into root\n",
    "        text = [simplemma.lemmatize(token, simpLang) for token in text]\n",
    "        text = ' '.join(text)\n",
    "        textCol.append(text)\n",
    "    return textCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f38640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store processed text with other dataset info  for easier accessing\n",
    "def store_ProcessedData(origDataset, processedText, filename): \n",
    "    processedText = pd.Series(processedText, name = 'Text')\n",
    "    origDataset['Text'] = processedText\n",
    "    origDataset.to_csv(filename,  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f74ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing and storing English Dataset\n",
    "processedText_EN = processText(en_data)\n",
    "store_ProcessedData(en_data, processedText_EN, '../2. Model Fit, Train, Test/Processed Text-Dataset/Processed_EN_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20875d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing and storing Spanish Dataset\n",
    "processedText_SPN = spanProcessText(spn_data)\n",
    "store_ProcessedData(spn_data, processedText_SPN, '../2. Model Fit, Train, Test/Processed Text-Dataset/Processed_Spn_Data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
